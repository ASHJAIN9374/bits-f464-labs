{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUzByS2TnMG8"
      },
      "source": [
        "# Lab 3 Take Home Assignment - Molecule Generation with Normalizing Flows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUDo5EennRHL"
      },
      "source": [
        "## What are Normalizing Flows:\n",
        "Normalizing Flows constitute a generative model that uses invertible neural networks to build a probability distribution. This methodology enables the computation of likelihoods and the generation of samples by transforming simple base distributions (e.g., gaussian) into more complex ones through a sequence of reversible transformations. The precision and control afforded by Normalizing Flows make them particularly advantageous for generating molecules with highly specific attributes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we will be using [SMILES](https://www.cs.tufts.edu/comp/150CSB/refs/1987%20%20SMILES,%20a%20chemical%20language%20and%20information%20system.%201.%20Introduction%20to%20methodology%20and%20encoding%20rules.pdf) [1] strings to represent molecules. SMILES (Simplified Molecular Input Line Entry System) is a string notation for representing molecules and reactions. It is a human-readable and compact way to represent a chemical structure.\n",
        "\n",
        "We will also be using [SELFIES](https://arxiv.org/abs/1905.13741) (SELF-referencIng Embedded Strings) [2] as the base distribution for our Normalizing Flow. It is a syntax that allows the representation of a molecule as a string. SELFIES is a more robust and flexible alternative to SMILES. It is a fully fledged molecular string representation that can be used to represent any molecule."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will be using the [RDKit](https://www.rdkit.org/) library to work with SMILES strings. RDKit is a collection of cheminformatics and machine learning tools that can be used to work with chemical data. It is a powerful library that can be used to work with SMILES strings, molecular fingerprints, and molecular descriptors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this assignment, You will have to code up the 2 main components of a normalizing flow:\n",
        "1. The Affine transformation\n",
        "2. The Normalizing flow\n",
        "\n",
        "You will also have to implement the training loop for the normalizing flow.\n",
        "\n",
        "The rest of the code is provided to you. You will have to fill in the missing parts of the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXtIU9W7n2X0"
      },
      "source": [
        "## Imports and Installs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd50P_v1n8-5"
      },
      "source": [
        "(yes there are a lot of things we are importing, you can safely ignore all of these as well as any warnings that might pop up with it)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4gJ7yJCoeTj"
      },
      "source": [
        "### Install packages that are not present on Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnTHkRFYwt3T"
      },
      "outputs": [],
      "source": [
        "!pip install --pre deepchem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O01MdK_V0bcc",
        "outputId": "4d49efa1-27ba-49b8-ba85-f9ff4aa96444"
      },
      "outputs": [],
      "source": [
        "!pip install selfies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klo6vOAgoi5a"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvxWlGsCoZ-X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Sequence, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nWs3-DCtyYc",
        "outputId": "227cfad3-1964-4b8e-c3eb-586d8643a411"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "\n",
        "torch.manual_seed(21) # Setting Seed for reproducability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaAaRzxHoSBC"
      },
      "outputs": [],
      "source": [
        "# Typical Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpU9XV8WpG3I"
      },
      "source": [
        "We will be using deepchem to download and handle our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LM8YozTPoR-f"
      },
      "outputs": [],
      "source": [
        "import deepchem as dc\n",
        "from deepchem.models.optimizers import Adam\n",
        "from deepchem.data import NumpyDataset\n",
        "from deepchem.splits import RandomSplitter\n",
        "from deepchem.molnet import load_qm7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMaTJ4uapPFX"
      },
      "source": [
        "rdkit is used to handle molecular visualizations and to check molecule validity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7ZvXDjJo6fM"
      },
      "outputs": [],
      "source": [
        "import rdkit\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "from rdkit.Chem import Draw\n",
        "from rdkit import RDLogger\n",
        "RDLogger.DisableLog('rdApp.*')  # suppress error messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSdPFDopp2OP"
      },
      "source": [
        "We use IPython Display to display molecules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFTr8X4GpvVM"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are using selfies to convert a string represented molecule to an object that can be used to train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6KtRJFKpvSh"
      },
      "outputs": [],
      "source": [
        "import selfies as sf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Kl0CmNU09ew"
      },
      "source": [
        "\n",
        "\n",
        "# Normalizing Flow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Affine Transformation\n",
        "An affine transformation is one of the simplest yet powerful transformations used in normalizing flows. It applies a linear transformation (scale and shift) to the data, which can alter the distribution in a way that increases its complexity. The affine transformation is defined as:\n",
        "\n",
        "$y=x⋅e^{a}+b$\n",
        "\n",
        "where $x$ is the input, $y$ is the transformed output, $a$ (scale) and $b$ (shift) are the parameters of the transformation.\n",
        "\n",
        "For transformations in normalizing flows to be useful, they must be bijective (invertible), allowing us to compute both forward and backward transformations. Additionally, the computation of the log-Jacobian determinant is crucial for calculating the change in log-density under the transformation, which is necessary for training these models using maximum likelihood."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1: Implement the Affine Transformation\n",
        "Your task is to implement the `Affine` class in PyTorch, which performs an affine transformation as part of a normalizing flow. The class should include both the forward and inverse transformations, as well as the computation of the log-Jacobian determinant for both directions.\n",
        "\n",
        "**Requirements**\n",
        "1. Forward Transformation: Implement the forward method to transform an input tensor $x$ using the formula $y=x⋅e^{a}+b$. Also, compute the log-Jacobian determinant of this transformation.\n",
        "2. Inverse Transformation: Implement the inverse method to reverse the transformation, computing \n",
        "$x$ from $y$ using the formula $x=(y−b)/e^{a}$, and calculate the log-Jacobian determinant for the inverse transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_knye1VWtvK-"
      },
      "outputs": [],
      "source": [
        "class Affine(nn.Module):\n",
        "  \"\"\"Class which performs the Affine transformation.\n",
        "\n",
        "  This transformation is based on the affinity of the base distribution with\n",
        "  the target distribution. A geometric transformation is applied where\n",
        "  the parameters performs changes on the scale and shift of a function\n",
        "  (inputs).\n",
        "\n",
        "  Normalizing Flow transformations must be bijective in order to compute\n",
        "  the logarithm of jacobian's determinant. For this reason, transformations\n",
        "  must perform a forward and inverse pass.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, dim: int) -> None:\n",
        "    \"\"\"Create a Affine transform layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dim: int\n",
        "      Value of the Nth dimension of the dataset.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__()\n",
        "    self.dim = dim\n",
        "    self.scale = nn.Parameter(torch.zeros(self.dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(self.dim))\n",
        "\n",
        "  def forward(self, x: Sequence) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Performs a transformation between two different distributions. This\n",
        "    particular transformation represents the following function:\n",
        "    y = x * exp(a) + b, where a is scale parameter and b performs a shift.\n",
        "    This class also returns the logarithm of the jacobians determinant\n",
        "    which is useful when invert a transformation and compute the\n",
        "    probability of the transformation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : Sequence\n",
        "      Tensor sample with the initial distribution data which will pass into\n",
        "      the normalizing flow algorithm.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    y : torch.Tensor\n",
        "      Transformed tensor according to Affine layer with the shape of 'x'.\n",
        "    log_det_jacobian : torch.Tensor\n",
        "      Tensor which represents the info about the deviation of the initial\n",
        "      and target distribution.\n",
        "\n",
        "    \"\"\"\n",
        "    ##############CODE STARTS HERE################\n",
        "    y = \n",
        "    det_jacobian = \n",
        "    log_det_jacobian = \n",
        "    ##############CODE ENDS HERE################\n",
        "    return y, log_det_jacobian\n",
        "\n",
        "  def inverse(self, y: Sequence) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Performs a transformation between two different distributions.\n",
        "    This transformation represents the bacward pass of the function\n",
        "    mention before. Its mathematical representation is x = (y - b) / exp(a)\n",
        "    , where \"a\" is scale parameter and \"b\" performs a shift. This class\n",
        "    also returns the logarithm of the jacobians determinant which is\n",
        "    useful when invert a transformation and compute the probability of\n",
        "    the transformation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y : Sequence\n",
        "      Tensor sample with transformed distribution data which will be used in\n",
        "      the normalizing algorithm inverse pass.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    x : torch.Tensor\n",
        "      Transformed tensor according to Affine layer with the shape of 'y'.\n",
        "    inverse_log_det_jacobian : torch.Tensor\n",
        "      Tensor which represents the information of the deviation of the initial\n",
        "      and target distribution.\n",
        "\n",
        "    \"\"\"\n",
        "    ##############CODE STARTS HERE################\n",
        "    x = \n",
        "    det_jacobian = \n",
        "    inverse_log_det_jacobian = \n",
        "    ##############CODE ENDS HERE################\n",
        "    return x, inverse_log_det_jacobian\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Normalizing Flow\n",
        "Normalizing flows model complex probability distributions through a sequence of invertible transformations. Starting from a simple distribution, such as a Multivariate Normal, each step transforms the distribution in a way that becomes more complex and can better approximate the target distribution. The key feature of normalizing flows is their invertibility and the tractability of their Jacobian determinant, allowing for exact likelihood computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task2 : Implementing the NormalizingFlow Class\n",
        "Your task is to implement the NormalizingFlow class, which will encapsulate the process of transforming a base distribution into a complex one through a sequence of transformations. This class should be able to compute the log probability of given inputs under the transformed distribution and generate samples from the transformed distribution.\n",
        "\n",
        "**Requirements**\n",
        "1. Log Probability Calculation: Implement the log_probability method to compute the log probability of inputs under the transformed distribution.\n",
        "2. Sampling: Implement the sample method to generate samples from the transformed distribution and compute their log probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRjaZJAkt8OZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NormalizingFlow(nn.Module):\n",
        "  \"\"\"Normalizing flows are widley used to perform generative models.\n",
        "  This algorithm gives advantages over variational autoencoders (VAE) because\n",
        "  of ease in sampling by applying invertible transformations\n",
        "  (Frey, Gadepally, & Ramsundar, 2022).\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, transform: Sequence, base_distribution, dim: int) -> None:\n",
        "    \"\"\"This class considers a transformation, or a composition of transformations\n",
        "    functions (layers), between a base distribution and a target distribution.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    transform: Sequence\n",
        "      Bijective transformation/transformations which are considered the layers\n",
        "      of a Normalizing Flow model.\n",
        "    base_distribution: torch.Tensor\n",
        "      Probability distribution to initialize the algorithm. The Multivariate Normal\n",
        "      distribution is mainly used for this parameter.\n",
        "    dim: int\n",
        "      Value of the Nth dimension of the dataset.\n",
        "\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.dim = dim\n",
        "    self.transforms = nn.ModuleList(transform)\n",
        "    self.base_distribution = base_distribution\n",
        "\n",
        "  def log_probability(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"This method computes the probability of the inputs when\n",
        "    transformation/transformations are applied.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    inputs: torch.Tensor\n",
        "      Tensor used to evaluate the log_prob computation of the learned\n",
        "      distribution.\n",
        "      shape: (samples, dim)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    log_prob: torch.Tensor\n",
        "      This tensor contains the value of the log probability computed.\n",
        "      shape: (samples)\n",
        "\n",
        "    \"\"\"\n",
        "    ##############CODE STARTS HERE################\n",
        "    log_prob = # Initialize log_prob \n",
        "    for biject in reversed(self.transforms):\n",
        "      inputs, inverse_log_det_jacobian = \n",
        "      log_prob =\n",
        "\n",
        "    log_prob = \n",
        "    ##############CODE ENDS HERE################\n",
        "    return log_prob\n",
        "\n",
        "  def sample(self, n_samples: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Performs a sampling from the transformed distribution.\n",
        "    Besides the outputs (sampling), this method returns the logarithm of\n",
        "    probability to obtain the outputs at the base distribution.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_samples: int\n",
        "      Number of samples to select from the transformed distribution\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    sample: tuple\n",
        "      This tuple contains a two torch.Tensor objects. The first represents\n",
        "      a sampling of the learned distribution when transformations had been\n",
        "      applied. The secong torc.Tensor is the computation of log probabilities\n",
        "      of the transformed distribution.\n",
        "      shape: ((samples, dim), (samples))\n",
        "\n",
        "    \"\"\"\n",
        "    ##############CODE STARTS HERE################\n",
        "    outputs = \n",
        "    log_prob = \n",
        "\n",
        "    for biject in self.transforms:\n",
        "      outputs, log_det_jacobian = \n",
        "      log_prob = \n",
        "    ##############CODE ENDS HERE################\n",
        "    return outputs, log_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MThPcAwM1JBT"
      },
      "source": [
        "# Train Function\n",
        "\n",
        "### Task 3: Implement the train function\n",
        "Your task is to implement the `train` function for the `NormalizingFlow` model. This function will perform gradient descent using the Adam optimizer to minimize the negative log-likelihood of the data. The training loop should include the following steps:\n",
        "1. Compute the log probability of the training data under the transformed distribution.\n",
        "2. Compute the negative log likelihood loss.\n",
        "3. Compute the gradients and update the model parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uv1fNpPAuY73"
      },
      "outputs": [],
      "source": [
        "def train(model, data, epochs = 100, batch_size = 64):\n",
        "    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    losses = []\n",
        "    with tqdm.tqdm(range(epochs), unit=' Epoch') as tepoch:\n",
        "        epoch_loss = 0\n",
        "        for epoch in tepoch:\n",
        "            for batch_index, training_sample in enumerate(train_loader):\n",
        "                ##############CODE STARTS HERE################\n",
        "                log_prob = \n",
        "\n",
        "                loss = \n",
        "\n",
        "\n",
        "                # ~ 3 lines of code to perform backpropagation\n",
        "\n",
        "                epoch_loss = \n",
        "                ##############CODE ENDS HERE################\n",
        "            epoch_loss /= len(train_loader)\n",
        "            losses.append(np.copy(epoch_loss.detach().numpy()))\n",
        "            tepoch.set_postfix(loss=epoch_loss.detach().numpy())\n",
        "    return model, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBbKwulty74q"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Downloading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_vPhbddw-VO"
      },
      "outputs": [],
      "source": [
        "# Download from MolNet\n",
        "tasks, datasets, transformers = dc.molnet.load_qm7(featurizer='ECFP')\n",
        "df = pd.DataFrame(data={'smiles': datasets[0].ids})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sampling from the dataset for training on colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDeqRQI-rVEo"
      },
      "outputs": [],
      "source": [
        "data = df[['smiles']].sample(2500, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setup SELFIES and Helper Functions (can be safely ignored)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRQn292JrmTx",
        "outputId": "7ddb3ff9-efb3-4d9e-a536-389d4eaf9416"
      },
      "outputs": [],
      "source": [
        "sf.set_semantic_constraints()  # reset constraints\n",
        "constraints = sf.get_semantic_constraints()\n",
        "constraints['?'] = 3\n",
        "\n",
        "sf.set_semantic_constraints(constraints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWsMHZNhro9j"
      },
      "outputs": [],
      "source": [
        "def preprocess_smiles(smiles):\n",
        "  return sf.encoder(smiles)\n",
        "\n",
        "def keys_int(symbol_to_int):\n",
        "  d={}\n",
        "  i=0\n",
        "  for key in symbol_to_int.keys():\n",
        "    d[i]=key\n",
        "    i+=1\n",
        "  return d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adding selfies and the length of the smiles to the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data['selfies'] = data['smiles'].apply(preprocess_smiles)\n",
        "data['len'] = data['smiles'].apply(lambda x: len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "AylgsngVrr7L",
        "outputId": "d888b1b0-0d10-4f8f-f3dc-1084fbc14ecc"
      },
      "outputs": [],
      "source": [
        "data.sort_values(by='len').head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7gswtq7rvru",
        "outputId": "34c12062-fc87-4b11-f2ce-2d3a7cc136d2"
      },
      "outputs": [],
      "source": [
        "selfies_list = np.asanyarray(data.selfies) # list of the selfies\n",
        "selfies_alphabet = sf.get_alphabet_from_selfies(selfies_list) # Get the alphabet from the list of selfies\n",
        "selfies_alphabet.add('[nop]')  # Add the \"no operation\" symbol as a padding character\n",
        "selfies_alphabet.add('.') \n",
        "selfies_alphabet = list(sorted(selfies_alphabet)) # Sort the alphabet\n",
        "largest_selfie_len = max(sf.len_selfies(s) for s in selfies_list) # Get the length of the largest selfies\n",
        "symbol_to_int = dict((c, i) for i, c in enumerate(selfies_alphabet)) # Create a symbol to integer mapping\n",
        "int_mol=keys_int(symbol_to_int) # Create a integer to symbol mapping\n",
        "print(largest_selfie_len) # Print the length of the largest selfies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Converting the SELFIES to a tensor by one hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6h85Dnkr2ad"
      },
      "outputs": [],
      "source": [
        "onehots=sf.batch_selfies_to_flat_hot(selfies_list, symbol_to_int,largest_selfie_len)\n",
        "input_tensor = torch.tensor(onehots, dtype=torch.float64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating Noise Tensor and Dequantized Tensor - Dequantization is a technique used to improve the quality of the generated samples. It involves adding noise to the input data to make the model more robust to small perturbations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Whnfo4IJuJzl"
      },
      "outputs": [],
      "source": [
        "noise_tensor = torch.rand(input_tensor.shape, dtype=torch.float64)\n",
        "dequantized_data = torch.add(input_tensor, noise_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Selecting only the first 35 columns of the one hot encoded tensor for sped up training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWvf2BxBvvkP"
      },
      "outputs": [],
      "source": [
        "dequantized_data = dequantized_data[:, :35]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating the Dataset and splitting the data into training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfuyPbeSwAU1",
        "outputId": "47812bc6-12d3-4066-aec8-59fb88f8a78c"
      },
      "outputs": [],
      "source": [
        "ds = NumpyDataset(dequantized_data)  # Create a DeepChem dataset\n",
        "splitter = RandomSplitter()\n",
        "train_s, val, test = splitter.train_valid_test_split(dataset=ds, seed=42)\n",
        "train_idx, val_idx, test_idx = splitter.split(dataset=ds, seed=42)\n",
        "\n",
        "dim = len(train_s.X[0])  # length of one-hot encoded vectors\n",
        "train_s.X.shape  # 2000 samples,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI1seGcPwmC4"
      },
      "outputs": [],
      "source": [
        "# SMILES strings of training data\n",
        "train_smiles = data['smiles'].iloc[train_idx].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNCsymP5wzvb",
        "outputId": "40df9310-3852-4738-f42a-f98f00e93352"
      },
      "outputs": [],
      "source": [
        "transforms = [Affine(dim)]\n",
        "distribution = MultivariateNormal(torch.zeros(dim), torch.eye(dim))\n",
        "\n",
        "model = NormalizingFlow(transforms, distribution, dim)\n",
        "model, loss = train(model, dequantized_data, epochs = 500)\n",
        "# plot_density(model, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample Molecules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKsruJBBcWg9"
      },
      "outputs": [],
      "source": [
        "generated_samples, _ = model.sample(10)\n",
        "mols = torch.floor(generated_samples)\n",
        "mols = torch.clamp(mols, 0, 1)\n",
        "mols_list = mols.detach().numpy().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbEVfxOreHyt"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "d = dict(itertools.islice(int_mol.items(), 35))\n",
        "mols=sf.batch_flat_hot_to_selfies(mols_list, d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmBMoj1Km4eJ"
      },
      "source": [
        "## Checking % of valid molecules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we will check the percentage of valid molecules in the generated samples\n",
        "\n",
        "First, we have to convert the molecules from the selfies representation to the SMILES representation, then we will use RDKit to check if the molecule is valid or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWn71XHftdWI",
        "outputId": "a3a6912b-d7af-48a7-d32c-812297105ffc"
      },
      "outputs": [],
      "source": [
        "valid_count = 0\n",
        "valid_selfies, invalid_selfies = [], []\n",
        "for idx, selfies in enumerate(mols):\n",
        "  try:\n",
        "    if Chem.MolFromSmiles(sf.decoder(mols[idx]), sanitize=True) is not None:\n",
        "        valid_count += 1\n",
        "        valid_selfies.append(selfies)\n",
        "    else:\n",
        "      invalid_selfies.append(selfies)\n",
        "  except Exception:\n",
        "    pass\n",
        "print('%.2f' % (valid_count / len(mols)),  ' of generated samples are valid molecules.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzZegM9Jtozu"
      },
      "outputs": [],
      "source": [
        "valid_smiles = [sf.decoder(vs) for vs in valid_selfies]\n",
        "gen_mols = [Chem.MolFromSmiles(sf.decoder(vs)) for vs in valid_selfies]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSAIbwoumMaw"
      },
      "source": [
        "## Helper Functions to Visualize Molecules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSc5irOetr7-"
      },
      "outputs": [],
      "source": [
        "def display_images(filenames):\n",
        "    \"\"\"Helper to pretty-print images.\"\"\"\n",
        "    for file in filenames:\n",
        "      display(Image(file))\n",
        "\n",
        "def mols_to_pngs(mols, basename=\"generated_mol\"):\n",
        "    \"\"\"Helper to write RDKit mols to png files.\"\"\"\n",
        "    filenames = []\n",
        "    for i, mol in enumerate(mols):\n",
        "        filename = \"%s%d.png\" % (basename, i)\n",
        "        Draw.MolToFile(mol, filename)\n",
        "        filenames.append(filename)\n",
        "    return filenames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QJ_32zMntu4y",
        "outputId": "36a2ab84-5fbe-425e-c67b-f220b02076f2"
      },
      "outputs": [],
      "source": [
        "display_mols = []\n",
        "for i in range(10):\n",
        "  display_mols.append(gen_mols[i])\n",
        "\n",
        "display_images(mols_to_pngs(display_mols))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References:\n",
        "\n",
        "[1] Weininger, D. (1988). SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28(1), 31-36.\n",
        "\n",
        "[2] Frey, N. C., Gadepally, V., & Ramsundar, B. (2022). Fastflows: Flow-based models for molecular graph generation. arXiv preprint arXiv:2201.12419.\n",
        "\n",
        "[3] Madhawa, K., Ishiguro, K., Nakago, K., & Abe, M. (2019). Graphnvp: An invertible flow model for generating molecular graphs. arXiv preprint arXiv:1905.11600.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This Lab was made by [Shreyas V](https://github.com/shreyasvinaya/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "6Kl0CmNU09ew"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
